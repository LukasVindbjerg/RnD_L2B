{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Load the custom vocabulary from the CSV file\n",
    "custom_vocab = []\n",
    "with open('data/custom_vocab.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header\n",
    "    for row in reader:\n",
    "        custom_vocab.append(row[0])\n",
    "\n",
    "# Print the custom vocabulary to verify\n",
    "# print(custom_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50324, 768)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Add custom tokens to the tokenizer\n",
    "tokenizer.add_tokens(custom_vocab)\n",
    "\n",
    "# Add a padding token to the tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Resize the model embeddings to match the new tokenizer size\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "# Define the dataset (this is just an example; use your actual data here)\n",
    "train_texts = [\"hello world\", \"my custom vocabulary is cool\"]\n",
    "\n",
    "# Encode the training data with padding\n",
    "train_encodings = tokenizer(train_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file using pandas\n",
    "df = pd.read_csv('data/dsl_training.csv')\n",
    "\n",
    "# Extract the input and output columns\n",
    "train_texts = df['input'].tolist()\n",
    "train_labels = df['output'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# Define the dataset (this is just an example; use your actual data here)\n",
    "train_texts = [\"hello world\", \"my custom vocabulary is cool\"]\n",
    "\n",
    "# Encode the training data\n",
    "train_encodings = tokenizer(train_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Shift the labels to the right to create the labels tensor\n",
    "labels = train_encodings['input_ids'].clone()\n",
    "labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss computation\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, labels)\n",
    "\"\"\"\n",
    "\n",
    "# # Add padding token if necessary\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "# Encode the training data with padding\n",
    "train_encodings = tokenizer(train_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Encode the labels\n",
    "label_encodings = tokenizer(train_labels, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Shift the labels to the right to create the labels tensor\n",
    "labels = label_encodings['input_ids'].clone()\n",
    "labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss computation\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, labels)\n",
    "\n",
    "# Define data collator to handle padding dynamically\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # We are not doing masked language modeling\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/369 [00:00<?, ?it/s]C:\\Users\\lvind\\AppData\\Local\\Temp\\ipykernel_20488\\1233769840.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|██████████| 369/369 [09:59<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 599.0559, 'train_samples_per_second': 1.227, 'train_steps_per_second': 0.616, 'train_loss': 4.3097095851329605, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./models/custom_vocab_tokenizer\\\\tokenizer_config.json',\n",
       " './models/custom_vocab_tokenizer\\\\special_tokens_map.json',\n",
       " './models/custom_vocab_tokenizer\\\\vocab.json',\n",
       " './models/custom_vocab_tokenizer\\\\merges.txt',\n",
       " './models/custom_vocab_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained('./models/custom_vocab_model')\n",
    "tokenizer.save_pretrained('./models/custom_vocab_tokenizer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lvind\\AppData\\Local\\Temp\\ipykernel_20488\\1233769840.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.080677032470703, 'eval_runtime': 0.1092, 'eval_samples_per_second': 18.322, 'eval_steps_per_second': 9.161, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define some test data\n",
    "test_texts = [\"hello my world\", \"vocabulary is custom\"]\n",
    "\n",
    "# Encode the test data\n",
    "test_encodings = tokenizer(test_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Use the trainer to evaluate the model\n",
    "results = trainer.evaluate(eval_dataset=CustomDataset(test_encodings, test_encodings['input_ids']))\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trash the  car  and clean the  bathroom   floor  and polish the  floor  and polish the  floor   table  and polish the  floor   table  and polish the  floor   floor  and polish the  floor   floor  and polish the  floor\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('./models/custom_vocab_model')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./models/custom_vocab_tokenizer')\n",
    "\n",
    "# Generate text\n",
    "input_text = \"trash\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Empty the trash\n",
      "Output: sink floor table table table table table table table table table table table table table table\n",
      "\n",
      "Input: Clean the kitchen sink\n",
      "Output: kitchen sink sink sink sink floor sink floor carpet carpet floor carpet carpet floor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('./models/custom_vocab_model')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./models/custom_vocab_tokenizer')\n",
    "\n",
    "\n",
    "def generate_custom_text(input_text, tokenizer, model, custom_tokens):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "    # Generate tokens with the model\n",
    "    output_ids = model.generate(input_ids, max_length=50)\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Ensure the generated text uses only custom vocabulary\n",
    "    tokens = generated_text.split()\n",
    "    filtered_tokens = [token for token in tokens if token in custom_tokens]\n",
    "    \n",
    "    # Join filtered tokens to form the final output\n",
    "    final_output = ' '.join(filtered_tokens)\n",
    "    return final_output\n",
    "\n",
    "# Define some test data\n",
    "test_texts = [\"Empty the trash\", \"Clean the kitchen sink\"]\n",
    "\n",
    "# Test the model with the custom generation function\n",
    "for text in test_texts:\n",
    "    generated_text = generate_custom_text(text, tokenizer, model, custom_vocab)\n",
    "    print(f\"Input: {text}\\nOutput: {generated_text}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RnD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
